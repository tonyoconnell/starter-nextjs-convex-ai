# Story 4.3: Implement the RAG-Powered Chat Backend

## Status

Ready

## Story

**As a** developer,
**I want** to connect the chat UI to a backend that uses RAG to generate context-aware responses,
**so that** the chatbot can answer questions about our project using the ingested knowledge base.

## Acceptance Criteria

1. The `AI Tool Service` (using the Vercel AI SDK) is configured with a `queryKnowledgeBase` tool.
2. When the user sends a message, this tool is called.
3. The tool generates an embedding for the user's query and searches the Vectorize DB for relevant document chunks.
4. These chunks are compiled into a context block and sent to the Claude API along with the original user query.
5. The response from Claude is streamed back to the frontend UI.

## Estimation & Planning

### Story Points

13

### Estimated Complexity

High

### Estimated Time

4-5 days

### Risk Level

Medium-High

## Tasks / Subtasks

- [ ] Task 1: Configure Vercel AI SDK with Tool Calling (AC: 1)
  - [ ] Install and configure Vercel AI SDK in the Convex backend
  - [ ] Create `queryKnowledgeBase` tool definition with proper schema
  - [ ] Implement tool parameter validation and error handling
  - [ ] Test tool registration and basic functionality

- [ ] Task 2: Implement RAG Query Processing (AC: 2, 3)
  - [ ] Create action to handle user chat messages with RAG integration
  - [ ] Generate embeddings for user queries using existing text processing
  - [ ] Query Vectorize DB for semantically similar document chunks
  - [ ] Implement relevance scoring and filtering for search results
  - [ ] Test query embedding generation and vector search functionality

- [ ] Task 3: Context Assembly and Prompt Engineering (AC: 4)
  - [ ] Compile relevant chunks into coherent context blocks
  - [ ] Design system prompts for context-aware responses
  - [ ] Implement context truncation for token limit management
  - [ ] Add metadata inclusion (source files, confidence scores)
  - [ ] Test context assembly with various query types and result sets

- [ ] Task 4: Claude API Integration with Streaming (AC: 5)
  - [ ] Configure Claude SDK for streaming responses
  - [ ] Implement tool calling workflow with RAG context
  - [ ] Handle streaming response chunks and real-time updates
  - [ ] Add proper error handling for API failures and timeouts
  - [ ] Test streaming responses with various query complexities

- [ ] Task 5: Frontend Integration and Real-time Updates (AC: 5)
  - [ ] Update chat UI to handle streaming responses
  - [ ] Implement loading states during RAG processing
  - [ ] Add typing indicators and progress feedback
  - [ ] Handle connection errors and graceful degradation
  - [ ] Test end-to-end chat flow with RAG responses

- [ ] Task 6: Performance Optimization and Caching
  - [ ] Implement query result caching for common questions
  - [ ] Optimize vector search parameters for speed vs accuracy
  - [ ] Add query preprocessing for better search results
  - [ ] Implement response time monitoring and logging
  - [ ] Test performance with various query types and knowledge base sizes

- [ ] Task 7: Comprehensive Testing and Error Handling
  - [ ] Unit tests for RAG query processing logic
  - [ ] Integration tests for complete chat flow
  - [ ] Test error scenarios (API failures, empty results, etc.)
  - [ ] Test with authenticated and anonymous users
  - [ ] Validate tool calling functionality and parameter passing

- [ ] Task 8: Security and Access Control Validation
  - [ ] Ensure authenticated user context is properly used
  - [ ] Validate access control for knowledge base queries
  - [ ] Implement rate limiting for RAG queries
  - [ ] Add audit logging for all RAG interactions
  - [ ] Test security edge cases and unauthorized access attempts

## Documentation Impact Assessment

**Architectural Patterns Established:**

- RAG (Retrieval Augmented Generation) implementation patterns
- Vercel AI SDK tool calling integration with Convex
- Vector search to LLM context assembly workflows
- Streaming AI responses with real-time frontend updates
- Context-aware prompt engineering and token management
- Tool-based AI agent architecture patterns

**Documentation Updates Needed:**

- Add RAG implementation guide to `docs/technical-guides/`
- Update `docs/architecture/api-implementation-details.md` with RAG endpoints
- Create AI tool calling patterns in `docs/patterns/backend-patterns.md`
- Document context assembly and prompt engineering strategies
- Add vector search optimization guide for performance tuning
- Update chat flow documentation with RAG integration

**Knowledge Capture:**

- RAG implementation best practices and performance considerations
- Vercel AI SDK integration patterns with Convex backend
- Vector search optimization techniques for relevance and speed
- Context assembly strategies for token efficiency
- Streaming response handling and real-time UI updates
- Tool calling architecture for extensible AI agents

**Examples to Create:**

- RAG query processing workflow examples
- Tool calling implementation patterns with Vercel AI SDK
- Context assembly and prompt engineering examples
- Streaming response integration examples
- Vector search optimization examples
- Security and access control patterns for AI endpoints

## Dev Notes

### Technical Architecture

From [Source: architecture/tech-stack.md]:

- **Vercel AI SDK** (Latest) - Powers in-app chatbot interface and tool-calling
- **Claude SDK** (Latest) - Direct backend communication with Claude models
- **Convex 1.12.x** - Real-time backend platform for Actions and tool integration
- **TypeScript 5.4.x** - Strict mode enforcement for type safety
- **Cloudflare Vectorize DB** - Vector storage and similarity search

### Previous Story Insights

From [Source: 4.1.story.md - Dev Agent Record]:

- Chat UI infrastructure complete with proper TypeScript types at `apps/web/types/chat.ts`
- Real-time message handling and loading states implemented
- Established patterns for message display and user interaction
- Error handling and connection state management in place

From [Source: 4.2.story.md - Dev Agent Record]:

- Knowledge ingestion service fully implemented with 300+ documents processed
- Vector embeddings stored in Cloudflare Vectorize with OpenAI text-embedding-3-small
- Text chunking and similarity search infrastructure ready
- Configuration management with OpenAI and OpenRouter API integration
- User access control implemented with graceful fallback for unauthorized users

From [Source: 4.1.5.story.md - Dev Agent Record]:

- BetterAuth + Convex integration complete for authenticated user context
- All Convex functions now receive proper user authentication instead of anonymous
- Session management and user identity mapping functional

### Data Models

From [Source: architecture/data-models.md]:

**Existing Chat Infrastructure:**

- `chat_sessions` table with user_id and title fields
- `chat_messages` table with session_id, user_id, role, and content
- Real-time subscriptions and message handling established

**Knowledge Base Integration:**

- `source_documents` table with file_path, content_hash, and processing_status
- `document_chunks` table with vectorize_id, content, and metadata
- Hybrid storage: Convex for metadata/content, Vectorize for embeddings

### API Specifications

From [Source: architecture/api-implementation-details.md]:

**Required RAG Action Signature:**

```typescript
// Main RAG-powered chat action
"agent:runRAGQuery": Action<{
  sessionId: Id<"chatSessions">;
  message: string;
  includeMetadata?: boolean;
  maxContextTokens?: number;
}> -> {
  response: string;
  sources: Array<{
    source_document: string;
    chunk_index: number;
    relevance_score: number;
  }>;
  processingTimeMs: number;
}

// Tool-based knowledge query
"tools:queryKnowledgeBase": Tool<{
  query: string;
  topK?: number;
  includeContent?: boolean;
}> -> {
  relevantChunks: Array<{
    content: string;
    source: string;
    score: number;
  }>;
}
```

**Integration Requirements:**

- Use Actions for tool calling and external API integration
- Maintain session context throughout conversation
- Include correlation IDs for debugging and monitoring
- Stream responses for real-time user experience

### File Locations

From [Source: architecture/source-tree.md]:

**New Files to Create:**

- `apps/convex/ragActions.ts` - RAG query processing and tool calling actions
- `apps/convex/tools/queryKnowledgeBase.ts` - Knowledge base tool implementation
- `apps/convex/lib/aiTools.ts` - Vercel AI SDK configuration and tool registry
- `apps/convex/lib/contextAssembly.ts` - Context compilation and prompt engineering
- `apps/web/lib/aiIntegration.ts` - Frontend AI integration utilities
- `apps/web/hooks/useRAGChat.ts` - React hook for RAG-powered chat
- `apps/web/components/chat/StreamingMessage.tsx` - Streaming response component

**Configuration Files:**

- Environment variables for Claude API configuration in `.env.local`
- Tool configuration in `apps/convex/lib/aiTools.ts`
- Chat streaming configuration in Next.js app

### Integration Patterns

From [Source: patterns/backend-patterns.md]:

**Vercel AI SDK Tool Pattern:**

```typescript
import { tool } from 'ai';

export const queryKnowledgeBaseTool = tool({
  description: 'Search the knowledge base for relevant information',
  parameters: z.object({
    query: z.string().describe('The search query'),
    topK: z.number().optional().describe('Number of results to return'),
  }),
  execute: async ({ query, topK = 5 }) => {
    // Vector search implementation
    const results = await vectorSearch(query, topK);
    return { relevantChunks: results };
  },
});
```

**RAG Action Pattern:**

```typescript
export const runRAGQuery = action({
  args: {
    sessionId: v.id('chatSessions'),
    message: v.string(),
    maxContextTokens: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    // 1. Generate query embedding
    // 2. Search vector database
    // 3. Assemble context
    // 4. Call Claude with tool
    // 5. Stream response back
    // 6. Store message in session
  },
});
```

**Streaming Response Pattern:**

```typescript
// Frontend streaming integration
const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat(
  {
    api: '/api/chat',
    onFinish: message => {
      // Handle completion
    },
    onError: error => {
      // Handle streaming errors
    },
  }
);
```

### Context Assembly Strategy

From [Source: 4.2.story.md - Implementation Notes]:

**Vector Search Configuration:**

- Use existing `queryVectorSimilarity` action from knowledge service
- OpenAI text-embedding-3-small for query embeddings (consistent with knowledge base)
- Cosine similarity search with configurable topK (default 5-10 results)
- Relevance score threshold to filter low-quality matches

**Context Compilation:**

```typescript
// Assemble context from search results
const contextBlock = assembleContext({
  chunks: searchResults,
  maxTokens: maxContextTokens || 4000,
  includeMetadata: true,
  sourceAttribution: true,
});

const systemPrompt = `You are an AI assistant with access to project documentation and code.
Use the following context to answer the user's question accurately:

${contextBlock}

Answer based on the provided context. If the context doesn't contain relevant information, say so clearly.`;
```

### Security and Access Control

From [Source: 4.1.5.story.md - Completion Notes]:

**User Authentication:**

- All RAG queries must use authenticated user context from BetterAuth integration
- Validate user access permissions before processing queries
- Include user_id in all logging and correlation tracking
- Graceful handling of anonymous users with appropriate messaging

**Rate Limiting and Monitoring:**

```typescript
// Rate limiting pattern for AI requests
const rateLimitCheck = await validateUserRateLimit(ctx, userId, 'rag_query');
if (!rateLimitCheck.allowed) {
  throw new ConvexError('Rate limit exceeded for RAG queries');
}

// Audit logging for all RAG interactions
await logRAGQuery({
  userId,
  query: args.message,
  resultsReturned: searchResults.length,
  processingTimeMs,
  correlationId,
});
```

### Performance Considerations

**Vector Search Optimization:**

- Use metadata filtering to scope search to relevant document types
- Implement query preprocessing (stemming, stop word removal)
- Cache frequent queries to reduce API calls
- Optimize chunk retrieval from Convex for speed

**Token Management:**

- Monitor context token usage to stay within Claude limits
- Implement intelligent context truncation preserving most relevant chunks
- Track token costs and usage patterns for optimization

### Testing Requirements

From [Source: architecture/test-strategy-and-standards.md]:

**Test File Locations:**

- `apps/convex/__tests__/ragActions.test.ts` - RAG action unit tests
- `apps/convex/__tests__/tools/queryKnowledgeBase.test.ts` - Tool unit tests
- `apps/convex/__tests__/contextAssembly.test.ts` - Context assembly tests
- `apps/web/__tests__/hooks/useRAGChat.test.ts` - Frontend hook tests
- `apps/web/__tests__/ragIntegration.e2e.test.ts` - End-to-end RAG tests

**Testing Standards:**

- Mock vector search responses for consistent test results
- Test streaming response handling and error scenarios
- Validate tool calling parameter passing and response handling
- Test authentication integration and access control
- Performance testing with various query complexities and context sizes

**Coverage Requirements:**

- 85%+ statements coverage for RAG processing logic
- Integration tests for complete chat-to-response workflow
- Error path testing for API failures, empty results, and invalid queries
- Authentication and authorization testing for all access patterns

### Technical Constraints

From [Source: architecture/coding-standards.md]:

**Mandatory Requirements:**

- No `any` types - use proper TypeScript interfaces for all tool definitions
- Include correlation IDs in all RAG operations for traceability
- Follow repository pattern for data access abstraction
- No direct `process.env` access - use centralized configuration

**Security Considerations:**

- Validate all user inputs before processing
- Sanitize query content to prevent prompt injection
- Rate limiting for AI API calls to prevent abuse
- Secure handling of API keys and sensitive configuration
- Access control validation for all knowledge base queries
- Input sanitization for AI prompts and context assembly

## Change Log

| Date       | Version | Description                 | Author                              |
| ---------- | ------- | --------------------------- | ----------------------------------- |
| 2025-01-26 | 1.0     | Initial story draft created | BMad Agent (create-next-story task) |

## Dev Agent Record

### Agent Model Used

_To be populated during implementation_

### Debug Log References

_To be populated during implementation_

### Completion Notes List

_To be populated during implementation_

### File List

_To be populated during implementation_

## QA Results

_Quality assurance results will be documented here after implementation._
